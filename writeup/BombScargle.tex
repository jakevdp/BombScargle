\documentclass[12pt,pdftex]{article}

\title{Robust Periodograms}
\author{Jake Vanderplas, \v{Z}eljko Ivezi\'{c}}

\begin{document}
\maketitle

\section{Lomb-Scargle Periodogram}
The Lomb-Scargle Periodogram is fundamentally a measure of a normalized chi-squared for fitting a sinusoidal model to data. Given data $\{t_j, y_j\}$ with homoscedastic Gaussian errors $\sigma$, and assuming that the mean was subtracted from ``raw'' data
values $\{y_j\}$, we can choose a single-term linear sinusoidal model defined by the frequency $\omega$ and amplitudes $a$ and $b$:

\begin{equation}
  f(t|\omega, a, b) \equiv a\sin(\omega t) + b\cos(\omega t)
\end{equation}

Given this model, we can write the likelihood

\begin{equation}
\label{eq:dataL} 
 L \equiv p(\{y_j\} |~\omega, a, b, \{t_j\}, \sigma) =
  \prod_{j=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(
  \frac{-[y_j - f(t_j|a, b, \omega)]^2}{2\sigma^2} \right)
\end{equation}

For any choice of $\omega$, we can find values $[a_0(\omega), b_0(\omega)]$ which maximize this likelihood. The goodness-of-fit can be determined by evaluating $\chi^2$ at this maximum:

\begin{equation}
  \chi^2(\omega) = \frac{1}{\sigma^2}\sum_{j=1}^N[y_j - f(t|\omega, a_0(\omega), b_0(\omega))]^2
\end{equation}

If we define $\chi_0^2 = \sigma^{-2}\sum y_j^2$, then we can write the {\it Lomb-Scargle Periodogram} as

\begin{equation}
\label{eq:PLS} 
  P_{LS}(\omega) \equiv 1 - \frac{\chi^2(\omega)}{\chi_0^2}
\end{equation}

This periodogram is a normalized measure of the goodness-of-fit of a sinusoidal model with frequency $\omega$, as compared to the null hypothesis of a pure-noise constant model, and lies in the range $0 \le P_{LS} \le 1$.

\section{Computing $P_{LS}$}
We can compute $P_{LS}$ quite easily via the above formalism.
For later convenience, let's re-express our model in the form of a matrix-vector product:
\begin{equation}
  f(t|\omega, \theta) = X_\omega \theta,
\end{equation}
where in the simple case above, $\theta = [a, b]^T$ and
\begin{equation}
  X_\omega = \left[\begin{array}{lll}
    \sin\omega t_1 && \cos\omega t_1\\
    \sin\omega t_2 && \cos\omega t_2\\
     & \vdots &\\
    \sin\omega t_N && \cos\omega t_N
  \end{array}\right]
\end{equation}

With this form, the $\chi^2$ can be concisely written
\begin{equation}
\chi^2(\omega, \theta) = \frac{1}{\sigma^2}||y - X_\omega \theta||^2
\end{equation}
This $\chi^2$ can be minimized by the standard means (assuming fixed $\omega$) to find the best-fit parameters:
\begin{equation}
\label{eq:thetaML}
  \theta_0 = (X_\omega^TX_\omega)^{-1}X_\omega^Ty.
\end{equation}
Plugging this result back in to the expression for $\chi^2$ gives
\begin{equation}
\label{eq:chi2}
  \chi^2(\omega) = \frac{1}{\sigma^2}\left[
    y^Ty - y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty
    \right]
\end{equation}
and comparing to the above definition we see that
\begin{equation}
  P_{LS}(\omega) = \frac{y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty}{y^Ty}
\end{equation}
Normally $P_{LS}$ is defined as some recipe of products of sines and cosines: all of that is contained in the above expression, and we won't repeat it here. Further, it is possible to use some tricks involving fast Fourier transforms to quickly compute the above expression for many frequencies $\omega$, but we won't get into that here.

The important point here is that fundamentally, the Lomb-Scargle periodogram is simply a normalized measure of the $\chi^2$ for a {\bf maximum-likelihood model fit} of a particular single-frequency periodic model. Further, it's clear that by changing the definition of $X_\omega$ and adding more columns, we can quite easily account for an arbitrary offset $\mu$ (as in the {\it generalized Lomb-Scargle method} proposed by Zechmeister \& Kurster 2009), include non-uniform errors $\sigma_j$, compute the periodogram for arbitrary multi-harmonic and multi-frequency models, etc. For detailed discussion, see Section 10.3 in ICVG2014. 

\section{Bayesian View}
The Bayesian view of this also starts with the likelihood.
We'll specify this in terms of $M_\omega$, which is our model for a given frequency $\omega$. In this case, the likelihood is:

\begin{equation}
  p(D|M_\omega,\theta) =
  (2\pi\sigma^2)^{-N/2} \exp\left(
  \frac{-||y - X_\omega\theta||^2}{2\sigma^2}
  \right)
\end{equation}
applying Bayes' rule and marginalizing over $\theta$ gives:
\begin{equation}
  p(M_\omega|D) = \frac{1}{p(D)}\int p(D|M_\omega,\theta)p(M_\omega, \theta){\rm d}^N\theta
\end{equation}
If we assume uniform priors (i.e. $p(M_\omega, \theta) \propto 1$) then it can be shown (see Appendix) that the Bayesian odds ratio between the model $M_\omega$ and the null model $M_0$ is given by:

\begin{equation}
  O_\omega \equiv \frac{p(M_\omega|D)}{p(M_0|D)} \propto \exp\left(P_{LS}(\omega)\frac{y^Ty}{2\sigma^2}\right)
\end{equation}

This is our Bayesian alternative to the frequentist Lomb-Scargle periodogram.
Maximizing $O_\omega$ across multiple values of $\omega$ gives us the best period in the Bayesian sense.
The nice piece here is that, unlike the Lomb-Scargle result which is predicated on $\chi^2$ as a goodness-of-fit, our Bayesian model allows the insertion of any likelihood, including ones which may be robust to outliers. We'll consider this below.

(JV: should we consider generalized LS from the beginning? in this case, the null model is not just $||y||^2$, but is $||y - \mu||^2$ where we solve for $\mu$: that is, $X_0 = [1, 1, \cdots 1]^T$. Nothing else needs to be modified, which is the nice part of this linear algebra approach!).

\section{Robust Periodogram based on Bayesian Approach}

A well-known problem with the Lomb-Scargle periodogram is its lack of robustness to outliers: the Gaussian form of the likelihood expression means that if the errors $\sigma_j$ are mis-specified, the outlying point(s) might have a large effect on the final fit. 
What is required is to replace the above $\chi^2$ computation with a robust model that can account for these errors.

If we knew which points were outliers, then we would simply exclude them and
apply standard Gaussian results to the remaining points (assuming that outliers
represent a small fraction of the data set). We will assume that we do not have this
information. Bayesian analysis enables a formal treatment of this problem, as well 
as the ability to estimate which points are likely outliers using an objective framework.

First, given $\{t_j, y_j, \sigma_j\}$, how do we assess whether non-Gaussianity is important? 
In case of no outliers, we expect that
\begin{equation}
           \chi^2_{\rm dof} = {1 \over N-1} \chi^2(\omega_0) \approx 1,
\end{equation}
where $\chi^2(\omega_0)$ is given by eq.~\ref{eq:chi2}, and evaluated at $\omega=\omega_0$ which
minimizes its value. If $\chi^2_{\rm dof}-1$ is a few times larger than $\sqrt{2/(N-1)}$, then it is unlikely
(as given by the cumulative pdf for $\chi^2_{\rm dof}$ distribution) that our data set $\{t_j, y_j\}$ was 
drawn from a distribution specified by the chosen model and Gaussian error distribution with 
the claimed $\{\sigma_j\}$.


\subsection{Bayesian Periodogram} 

We start by reformulating the data likelihood from eq.~\ref{eq:dataL}  as
\begin{eqnarray}
\label{eq:dataL2} 
    p(\{y_j, g_j\} |~\omega, \theta, \{t_j, \sigma_j\}) = \nonumber \\ 
  \prod_{j=1}^{N} \left[ \frac{g_j}{\sqrt{2\pi\sigma_j^2}} \exp\left(
  \frac{-[y_j - f(t_j|\omega, \theta)]^2}{2\sigma_j^2}\right) + 
       (1-g_j) p_{\rm bad}(y_j|I) \right].
\end{eqnarray}  
Here $g_j$ is 1 if the data point is ``good'' and 0 if it came from the distribution
of outliers, $p_{\rm bad}(y_j|I)$. In this model $p_{\rm bad}(y_j|I)$ applies to all
outliers. Again, if we knew $g_j$ this would be an easy problem to solve.

Since $\{g_j\}$ represent hidden variables, we shall treat them as model parameters and then
marginalize over them to get $p(\theta|\{y_j, t_j, \sigma_j\} ,I)$. With a separable prior,
which implies that the reliability of the measurements is decoupled from the true value
of the quantity we are measuring,
\begin{equation}
        p(\theta,\{g_j\}|I) = p(\theta|I)  \, p(\{g_j\}|I),
\end{equation}
we get
\begin{equation}
   p(\theta,\{g_j\}|~\{y_j, t_j, \sigma_j\}, I) \propto \prod_{j=1}^{N} \left[ g_j p_{\rm good}(y_j)   + (1-g_j) p_{\rm bad}(y_j|I) \right] p(\{g_j\}|I),
\end{equation}
where we assumed uniform priors for parameters $\theta$ and introduced for notational simplicity
\begin{equation}
       p_{\rm good}(y_j) = \frac{1}{\sqrt{2\pi\sigma_j^2}} \exp\left(
                  \frac{-[y_j - f(t_j|\omega, \theta)]^2}{2\sigma_j^2}\right). 
\end{equation}
Finally, marginalizing over $g_j$ gives
\begin{equation}
  p(\theta|~\{y_j, t_j, \sigma_j\}, I) \propto \int  p(\theta,\{g_j\}|~\{y_j, t_j, \sigma_j\}, I) \, d^N g_j.
\end{equation}


Following Section 5.6.7 in ICVG2014, in case of uniform priors for all $g_j$, marginalization over
$g_j$ effectively replaces every $g_j$ by 1/2 and leads to 
\begin{equation}
\label{eq:Btheta}
   p(\theta|~\{y_j, t_j, \sigma_j\}, I) \propto \prod_{j=1}^{N} \left[ p_{\rm good}(y_j)  + p_{\rm bad}(y_j|I) \right]. 
\end{equation}



\subsection{The Last Step?}

The posterior pdf for $\theta$ given by eq.~\ref{eq:Btheta} can be used to obtain the MAP value $\theta_0$.
By replacing $\theta_0$ from eq.~\ref{eq:thetaML} with this MAP estimate, the resulting robust 
$\chi^2(\omega)$ is used instead of $\chi^2(\omega)$ given by eq.~\ref{eq:chi2}, yielding a robust equivalent
of eq.~\ref{eq:PLS}. 

Is \v{Z} missing something here? I understand that the addition of $p_{\rm bad}$ in eq.~\ref{eq:Btheta}
prevents the use of closed-form solutions for $P_{LS}(\omega)$ (because taking the log of eq.~\ref{eq:Btheta} 
doesn't result in a linear problem). Is this the only problem? 

Can we use the same method here as used in the regression problem with straight line plus outliers? 

Should we perhaps think of a procedure similar to EM algorithm? 

 

\subsection{QA and Visualization} 

An obvious plot is to compare Lomb-Scargle and Bomb-Scargle periodograms in the same figure. 

One could make a 2D plot where the x axis is $\omega$ and the y axis is the $j$, the data point index.
Each ($\omega, j$) pixel gets colored by its MAP value of $g_j$. 

\section{Appendix}
Here is the calculation of the Bayes factor for our model.

For separable priors $p(M_\omega,\theta) = p(M_\omega)p(\theta)$ the posterior for our linear model is:

\begin{equation}
  p(M_\omega|D) = \frac{p(M_\omega)}{p(D)}\int{\rm d}^N\theta(2\pi\sigma^2)^{-N/2}\exp\left(\frac{-||y - X_\omega\theta||^2}{2\sigma^2}\right)
\end{equation}

We can compute this integral by completing the square in $\theta$. Let's look at the argument of the exponent:

\begin{equation}
  ||y - X_\omega\theta||^2 = \theta^TX_\omega^TX_\omega\theta - 2\theta^TX_\omega^Ty + y^Ty
\end{equation}

If we now define the hermitian matrix $C = X_\omega^TX_\omega$ and find its Cholesky decomposition $U^TU = C$, then we can rewrite this as

\begin{equation}
  ||y - X_\omega\theta||^2 = ||v - U\theta||^2 + y^Ty - v^Tv
\end{equation}

where the length-N array $v$ satisfies $U^Tv = X_\omega^Ty$. Given this, we can rewrite the expression

\begin{equation}
  ||y - X_\omega\theta||^2 = ||v - U\theta||^2 + y^Ty - y^TX_\omega C^{-1}X_\omega^Ty
\end{equation}

The expression $\phi = v - U\theta$ is now an $\mathbf{R}^N\to\mathbf{R}^N$ mapping with a Jacobian given by $U$, so we can change variables to $\phi$ in the above integral to simplify its evaluation:

\begin{equation}
  p(M_\omega|D) = \frac{p(M_\omega)}{p(D)}(2\pi\sigma^2)^{-N/2}
\int\frac{{\rm d}^N\phi}{\det|U|}\exp\left(\frac{-||\phi||^2 - y^Ty + y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty}{2\sigma^2}\right)
\end{equation}

Observing that $\det|U|^2 = \det|C| = \det|X_\omega^TX_\omega|$ and evaluating the (now straightforward) integral gives

\begin{equation}
  p(M_\omega|D) = \frac{p(M_\omega)}{p(D)}\frac{1}{\sqrt{\det|X_\omega^TX_\omega|}}
  \exp\left(\frac{y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty - y^Ty}{2\sigma^2}\right)
\end{equation}

Recalling that

\begin{equation}
  P_{LS}(\omega) = \frac{y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty}{y^Ty}
\end{equation}

we see that we can express this

\begin{equation}
  p(M_\omega|D) = \frac{p(M_\omega)}{p(D)}\frac{1}{\sqrt{\det|X_\omega^TX_\omega|}}
  \exp\left(\frac{y^Ty(P_{LS}(\omega) - 1)}{2\sigma^2}\right)
\end{equation}

We can compare two models using the odds ratio:
\begin{equation}
  O_{\omega_1\omega_2} \equiv \frac{p(M_{\omega_2}|D)}{p(M_{\omega_1}|D)}
  =\frac{p(M_{\omega_2})}{p(M_{\omega_1})}\sqrt{\frac{\det|X_{\omega_1}^TX_{\omega_1}|}{\det|X_{\omega_2}^TX_{\omega_2}|}}\exp\left(\frac{y^Ty}{2\sigma^2}\left[P_{LS}(\omega_2) - P_{LS}(\omega_1)\right]\right)
\end{equation}

For the special case of $M_0$, where $X = 0$, we can show
\begin{equation}
\frac{p(M_{\omega}|D)}{p(M_0|D)} =
\frac{p(M_{\omega})}{p(M_0)}\sqrt{\frac{2\pi\sigma^2}{\det|X_\omega^TX_\omega|}}\exp\left(\frac{y^Ty}{2\sigma^2}\left[P_{LS}(\omega) - 1\right]\right)
\end{equation}

\end{document}
