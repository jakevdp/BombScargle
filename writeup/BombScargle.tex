\documentclass[12pt,pdftex]{article}


\usepackage[usenames,dvipsnames]{color}
\newcommand{\jake}[1]{{\color{blue}\it[JTV: #1]}}
\newcommand{\zeljko}[1]{{\color{ForestGreen}\it[ZI: #1]}}
\newcommand{\problem}[1]{{\color{red}\it[problem: #1]}}

\title{Creating Robust Periodograms}
\author{Jake Vanderplas, \v{Z}eljko Ivezi\'{c}}

\begin{document}

\maketitle

\begin{abstract}
The Lomb-Scargle periodogram is a well-known method of analyzing periodicity in unevenly-sampled time-series data. The problem with the LS approach is that it is not robust to outliers in data. In this paper we propose and compare three approaches to computing robust periodograms: a procedural iterative drop-out approach, a frequentist approach based on robust loss functions, and a Bayesian approach based on marginalization over nuisance parameters.
\end{abstract}

\section{The Plan}
\begin{enumerate}
  \item Derive classic (generalized) Lomb-Scargle
  \item Demonstrate the non-robustness of the classic result
  \item Show the Bayesian formulation \& the robust mixture-model version
  \item Show the M-Estimator robust version (Huber's loss function)
  \item Show the iterative dropout engineering solution
  \item Compare results for robustness, computational complexity, etc.
\end{enumerate}

\section{The Lomb-Scargle Periodogram}
The Lomb-Scargle Periodogram is fundamentally a measure of a normalized chi-squared for fitting a sinusoidal model to data. Given data $\{t_j, y_j\}$ with homoscedastic Gaussian errors $\sigma$, and assuming that the mean was subtracted from ``raw'' data
values $\{y_j\}$, we can choose a single-term linear sinusoidal model defined by the frequency $\omega$ and amplitudes $a$ and $b$:

\begin{equation}
  f(t|\omega, a, b) \equiv a\sin(\omega t) + b\cos(\omega t)
\end{equation}

Given this model, we can write the likelihood

\begin{equation}
\label{eq:dataL} 
 L \equiv p(\{y_j\} |~\omega, a, b, \{t_j\}, \sigma) =
  \prod_{j=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(
  \frac{-[y_j - f(t_j|a, b, \omega)]^2}{2\sigma^2} \right)
\end{equation}

For any choice of $\omega$, we can find values $[a_0(\omega), b_0(\omega)]$ which maximize this likelihood. The goodness-of-fit can be determined by evaluating $\chi^2$ at this maximum:

\begin{equation}
  \chi^2(\omega) = \frac{1}{\sigma^2}\sum_{j=1}^N[y_j - f(t|\omega, a_0(\omega), b_0(\omega))]^2
\end{equation}

If we define $\chi_0^2 = \sigma^{-2}\sum y_j^2$, then we can write the {\it Lomb-Scargle Periodogram} as

\begin{equation}
\label{eq:PLS} 
  P_{LS}(\omega) \equiv 1 - \frac{\chi^2(\omega)}{\chi_0^2}
\end{equation}

This periodogram is a normalized measure of the goodness-of-fit of a sinusoidal model with frequency $\omega$, as compared to the null hypothesis of a pure-noise constant model, and lies in the range $0 \le P_{LS} \le 1$.

\subsection{Computing $P_{LS}$}
We can compute $P_{LS}$ quite easily via the above formalism.
For later convenience, let's re-express our model in the form of a matrix-vector product:
\begin{equation}
  f(t|\omega, \theta) = X_\omega \theta,
\end{equation}
where in the simple case above, $\theta = [a, b]^T$ and
\begin{equation}
  X_\omega = \left[\begin{array}{lll}
    \sin\omega t_1 && \cos\omega t_1\\
    \sin\omega t_2 && \cos\omega t_2\\
     & \vdots &\\
    \sin\omega t_N && \cos\omega t_N
  \end{array}\right]
\end{equation}
Letting $y = [y_1, y_2\cdots y_N]^T$ be the vector of amplitudes, the $\chi^2$ expression can be concisely written
\begin{equation}
\chi^2(\omega, \theta) = \frac{1}{\sigma^2}||y - X_\omega \theta||^2
\end{equation}
Assuming a fixed $\omega$, this $\chi^2$ can be minimized by standard means to find the best-fit parameters:
\begin{equation}
\label{eq:thetaML}
  \theta_0(\omega) = (X_\omega^TX_\omega)^{-1}X_\omega^Ty.
\end{equation}
Plugging this result back in to the expression for $\chi^2$ gives
\begin{equation}
\label{eq:chi2}
  \chi^2(\omega) = \frac{1}{\sigma^2}\left[
    y^Ty - y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty
    \right]
\end{equation}
The reference model (with $X=0$) gives $\chi_0^2 = \sigma^{-2}y^Ty$, so we see that
\begin{equation}
  P_{LS}(\omega) = \frac{y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty}{y^Ty}
\end{equation}
In the standard treatment, $P_{LS}$ is defined as some recipe of products of sines and cosines (e.g. Scargle 1982; Zechmeister \& Kurster 2009, ICVG2014): all of that is contained in the above expression, and we won't repeat it here. Further, it is possible to use some tricks involving fast Fourier transforms to quickly compute the above expression for many frequencies $\omega$, but we'll not get into those algorithmic considerations here.

The important point here is that fundamentally, the Lomb-Scargle periodogram is simply a normalized measure of the $\chi^2$ for a {\bf maximum-likelihood model fit} of a particular single-frequency periodic model. Further, it's clear that by changing the definition of $X_\omega$ and adding more columns, we can quite easily account for an arbitrary offset $\mu$ (as in the {\it floating-mean Lomb-Scargle method} proposed by Zechmeister \& Kurster 2009), include non-uniform errors $\sigma_j$, compute the periodogram for arbitrary multi-harmonic and multi-frequency models, etc. For detailed discussion, see Section 10.3 in ICVG2014.

\jake{Show an example periodogram \& folded light curve here}

\jake{Mention generalizations to multiple Fourier terms: Bretthorst 2003; Zechmeister \& Kurstur 2009. These are included in our formalism here!}

\jake{This all needs to be slightly modified for heteroscedastic errors. Should we do this from the beginning, or mention it separately?}

\subsection{Bayesian View of the Lomb-Scargle Periodogram}
\jake{Mention Bretthorst's work in this area}

The Bayesian view of this also starts with the likelihood.
We'll specify this in terms of $\omega$, which is our model for a given frequency $\omega$. In this case, the likelihood is:

\begin{equation}
  p(D|\omega,\theta) =
  (2\pi\sigma^2)^{-N/2} \exp\left(
  \frac{-||y - X_\omega\theta||^2}{2\sigma^2}
  \right)
\end{equation}
applying Bayes' rule and marginalizing over $\theta$ gives:
\begin{equation}
  p(\omega|D) = \frac{1}{p(D)}\int p(D|\omega,\theta)p(\omega, \theta){\rm d}^N\theta
\end{equation}
If we assume uniform priors (i.e. $p(\omega, \theta) \propto 1$) then it can be shown (see Appendix) that the Bayesian odds ratio between the model $\omega$ and the null model $M_0$ is given by:

\begin{equation}
  O_\omega \equiv \frac{p(\omega|D)}{p(M_0|D)} \propto \exp\left(\frac{\chi^2(\omega)}{2\sigma^2}\right) \propto \exp\left(\frac{\chi_0^2}{2\sigma^2}P_{LS}(\omega)\right)
\end{equation}


This is our Bayesian alternative to the frequentist Lomb-Scargle periodogram.
Maximizing $O_\omega$ across multiple values of $\omega$ gives us the best period in the Bayesian sense.
The beauty here is that, unlike the Lomb-Scargle result which is predicated on $\chi^2$ as a goodness-of-fit, our Bayesian model allows the insertion of any likelihood, including ones which may be robust to outliers. We'll consider this below.

\jake{should we consider floating-mean LS from the beginning? in this case, the null model is not just $||y||^2$, but is $||y - \mu||^2$ where we solve for $\mu$: that is, $X_0 = [1, 1, \cdots 1]^T$. Nothing else needs to be modified, which is the nice part of this linear algebra approach!}

\section{Non-Robustness of the Classic Lomb-Scargle}

\jake{Show a figure here, perhaps using LINEAR sample 1004849}

\section{Robust Periodogram: Huber Loss}
{\it Follow Huber 1981 and use a general $M$-estimator corresponding to the Huber Loss (Huber 1963).}

Huber (1981) proposed the $M$-estimator, which minimizes the generalized loss function
\begin{equation}
  \sum_i \rho(y_i|y).
\end{equation}
In the special case of the standard maximum-likelihood this loss function is proportional to the log of the likelihood:
\begin{equation}
  \rho(y_i|y) = \frac{(y_i - y)^2}{2\sigma_i^2}.
\end{equation}
This makes it clear that any outliers contribute quadratically to the loss, which is why they will have such a large effect on the fit.
One way to address this is to soften these tails for distant points. Huber (1963) proposed a softer loss function, usually known as the {\it Huber Loss}:
\begin{equation}
  \rho_c(y_i|y) = \left\{
  \begin{array}{ll}
    \frac{1}{2}t^2; & |t| \le c \\
    c|t| - \frac{1}{2}t^2; & |t| > c
  \end{array}
  \right.
\end{equation}
where we've defined $t_i \equiv (y_i - y) / \sigma_i$. Here $c$ is a free parameter which gives the cutoff (in units of $\sigma$) beyond which the loss function turns over.

If we replace the standard $\chi^2$ goodness-of-fit with an adjusted goodness-of-fit based on this Huber loss, the result is a more robust estimate of the periodogram.

\jake{I just realized that this procedure may actually increase the $\chi^2$ at the minima of the periodogram fairly significantly! We'll need to quantify the degree of this effect.}

\section{Robust Periodogram: Iterative Drop-outs}
ad-hoc procedural solution. Fit a model, drop outliers, repeat until it converges.

\jake{I just realized an issue: the drop-outs will be different for each $\omega$! That is, for the wrong $\omega$ the $\chi^2$ should be very large. Should there be a limit on the number of dropped points?}


\section{Robust Periodogram based on Bayesian Approach}

A well-known problem with the Lomb-Scargle periodogram is its lack of robustness to outliers: the Gaussian form of the likelihood expression means that if the errors $\sigma_j$ are mis-specified, the outlying point(s) might have a large effect on the final fit. 
What is required is to replace the above $\chi^2$ computation with a robust model that can account for these errors.

If we knew which points were outliers, then we would simply exclude them and
apply standard Gaussian results to the remaining points (assuming that outliers
represent a small fraction of the data set). We will assume that we do not have this
information. Bayesian analysis enables a formal treatment of this problem, as well 
as the ability to estimate which points are likely outliers using an objective framework.

First, given $\{t_j, y_j, \sigma_j\}$, how do we assess whether non-Gaussianity is important? 
In case of no outliers, we expect that
\begin{equation}
           \chi^2_{\rm dof} = {1 \over N-1} \chi^2(\omega_0) \approx 1,
\end{equation}
where $\chi^2(\omega_0)$ is given by eq.~\ref{eq:chi2}, and evaluated at $\omega=\omega_0$ which
minimizes its value. If $\chi^2_{\rm dof}-1$ is a few times larger than $\sqrt{2/(N-1)}$, then it is unlikely
(as given by the cumulative pdf for $\chi^2_{\rm dof}$ distribution) that our data set $\{t_j, y_j\}$ was 
drawn from a distribution specified by the chosen model and Gaussian error distribution with 
the claimed $\{\sigma_j\}$.


\subsection{Bayesian Periodogram} 

We start by reformulating the data likelihood from eq.~\ref{eq:dataL}  as
\begin{eqnarray}
\label{eq:dataL2} 
    p(\{y_j, g_j\} |~\omega, \theta, \{t_j, \sigma_j\}) = \nonumber \\ 
  \prod_{j=1}^{N} \left[ \frac{g_j}{\sqrt{2\pi\sigma_j^2}} \exp\left(
  \frac{-[y_j - f(t_j|\omega, \theta)]^2}{2\sigma_j^2}\right) + 
       (1-g_j) p_{\rm bad}(y_j|I) \right].
\end{eqnarray}  
Here $g_j$ is 1 if the data point is ``good'' and 0 if it came from the distribution
of outliers, $p_{\rm bad}(y_j|I)$. In this model $p_{\rm bad}(y_j|I)$ applies to all
outliers. Again, if we knew $g_j$ this would be an easy problem to solve.

Since $\{g_j\}$ represent hidden variables, we shall treat them as model parameters and then
marginalize over them to get $p(\theta|\{y_j, t_j, \sigma_j\} ,I)$. With a separable prior,
which implies that the reliability of the measurements is decoupled from the true value
of the quantity we are measuring,
\begin{equation}
        p(\theta,\{g_j\}|I) = p(\theta|I)  \, p(\{g_j\}|I),
\end{equation}
we get
\begin{equation}
   p(\theta,\{g_j\}|~\{y_j, t_j, \sigma_j\}, I) \propto \prod_{j=1}^{N} \left[ g_j p_{\rm good}(y_j)   + (1-g_j) p_{\rm bad}(y_j|I) \right] p(\{g_j\}|I),
\end{equation}
where we assumed uniform priors for parameters $\theta$ and introduced for notational simplicity
\begin{equation}
       p_{\rm good}(y_j) = \frac{1}{\sqrt{2\pi\sigma_j^2}} \exp\left(
                  \frac{-[y_j - f(t_j|\omega, \theta)]^2}{2\sigma_j^2}\right). 
\end{equation}
Finally, marginalizing over $g_j$ gives
\begin{equation}
  p(\theta|~\{y_j, t_j, \sigma_j\}, I) \propto \int  p(\theta,\{g_j\}|~\{y_j, t_j, \sigma_j\}, I) \, d^N g_j.
\end{equation}


Following Section 5.6.7 in ICVG2014, in case of uniform priors for all $g_j$, marginalization over
$g_j$ effectively replaces every $g_j$ by 1/2 and leads to 
\begin{equation}
\label{eq:Btheta}
   p(\theta|~\{y_j, t_j, \sigma_j\}, I) \propto \prod_{j=1}^{N} \left[ p_{\rm good}(y_j)  + p_{\rm bad}(y_j|I) \right]. 
\end{equation}

\jake{Show the expression for $p(\omega|~\{y_j, t_j, \sigma_j\}, I)$; this is what we compute via MCMC}

\section{Discussion}
\begin{enumerate}
  \item compare the three approaches
  \item compare results on several LINEAR curves
  \item discuss computational issues  
\end{enumerate}
 

\subsection{QA and Visualization} 

An obvious plot is to compare Lomb-Scargle and Bomb-Scargle periodograms in the same figure. 

One could make a 2D plot where the x axis is $\omega$ and the y axis is the $j$, the data point index.
Each ($\omega, j$) pixel gets colored by its MAP value of $g_j$.


\section{Conclusion}
This is what we did

\section*{References}


\appendix
\section{Deriving the Bayesian Expression}
Here is the calculation of the Bayes factor for our model.

For separable priors $p(\omega,\theta) = p(\omega)p(\theta)$ the posterior for our linear model is:

\begin{equation}
  p(\omega|D) = \frac{p(\omega)}{p(D)}\int{\rm d}^N\theta(2\pi\sigma^2)^{-N/2}\exp\left(\frac{-||y - X_\omega\theta||^2}{2\sigma^2}\right)
\end{equation}

We can compute this integral by completing the square in $\theta$. Let's look at the argument of the exponent:

\begin{equation}
  ||y - X_\omega\theta||^2 = \theta^TX_\omega^TX_\omega\theta - 2\theta^TX_\omega^Ty + y^Ty
\end{equation}

If we now define the hermitian matrix $C = X_\omega^TX_\omega$ and find its Cholesky decomposition $U^TU = C$, then we can rewrite this as

\begin{equation}
  ||y - X_\omega\theta||^2 = ||v - U\theta||^2 + y^Ty - v^Tv
\end{equation}

where the length-N array $v$ satisfies $U^Tv = X_\omega^Ty$. Given this, we can rewrite the expression

\begin{equation}
  ||y - X_\omega\theta||^2 = ||v - U\theta||^2 + y^Ty - y^TX_\omega C^{-1}X_\omega^Ty
\end{equation}

The expression $\phi = v - U\theta$ is now an $\mathbf{R}^N\to\mathbf{R}^N$ mapping with a Jacobian given by $U$, so we can change variables to $\phi$ in the above integral to simplify its evaluation:

\begin{equation}
  p(\omega|D) = \frac{p(\omega)}{p(D)}(2\pi\sigma^2)^{-N/2}
\int\frac{{\rm d}^N\phi}{\det|U|}\exp\left(\frac{-||\phi||^2 - y^Ty + y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty}{2\sigma^2}\right)
\end{equation}

Observing that $\det|U|^2 = \det|C| = \det|X_\omega^TX_\omega|$ and evaluating the (now straightforward) integral gives

\begin{equation}
  p(\omega|D) = \frac{p(\omega)}{p(D)}\frac{1}{\sqrt{\det|X_\omega^TX_\omega|}}
  \exp\left(\frac{y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty - y^Ty}{2\sigma^2}\right)
\end{equation}

Recalling that

\begin{equation}
  P_{LS}(\omega) = \frac{y^TX_\omega(X_\omega^TX_\omega)^{-1}X_\omega^Ty}{y^Ty}
\end{equation}

we see that we can express this

\begin{equation}
  p(\omega|D) = \frac{p(\omega)}{p(D)}\frac{1}{\sqrt{\det|X_\omega^TX_\omega|}}
  \exp\left(\frac{y^Ty(P_{LS}(\omega) - 1)}{2\sigma^2}\right)
\end{equation}

We can compare two models using the odds ratio:
\begin{equation}
  O_{\omega_1\omega_2} \equiv \frac{p(M_{\omega_2}|D)}{p(M_{\omega_1}|D)}
  =\frac{p(M_{\omega_2})}{p(M_{\omega_1})}\sqrt{\frac{\det|X_{\omega_1}^TX_{\omega_1}|}{\det|X_{\omega_2}^TX_{\omega_2}|}}\exp\left(\frac{y^Ty}{2\sigma^2}\left[P_{LS}(\omega_2) - P_{LS}(\omega_1)\right]\right)
\end{equation}

For the special case of $M_0$, where $X = 0$, we can show
\begin{equation}
\frac{p(M_{\omega}|D)}{p(M_0|D)} =
\frac{p(M_{\omega})}{p(M_0)}\sqrt{\frac{2\pi\sigma^2}{\det|X_\omega^TX_\omega|}}\exp\left(\frac{y^Ty}{2\sigma^2}\left[P_{LS}(\omega) - 1\right]\right)
\end{equation}

\end{document}
